{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8088b52c-4afa-4ae5-89fc-ac533318e991",
   "metadata": {},
   "source": [
    "# Cover Page\n",
    "\n",
    "## STUDENT ID No: 210006819\n",
    "\n",
    "## MODULE CODE: GG4257\n",
    "\n",
    "## MODULE TITLE: Urban Analytics: A Toolkit for Sustainable Urban Development\n",
    "\n",
    "## ASSIGNMENT: Independent Research Project\n",
    "\n",
    "## DEGREE PROGRAMME: International Relations and Geography\n",
    "\n",
    "## WORD COUNT: N/A\n",
    "\n",
    "## DEADLINE DATE: 05/01/2025\n",
    "\n",
    "In submitting this assignment I hereby confirm that:\n",
    "\n",
    "I have read the University's statement on Good Academic Practice; that the following work is my own work; and that significant academic debts and borrowings have been properly acknowledged and referenced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680f3ed-2777-4ad6-8828-894de66d14cd",
   "metadata": {},
   "source": [
    "# Reproducing the Code\n",
    "\n",
    "This section introduces how to replicate the code and how to get the required data. The code used to produce these maps can be found in a separate jupyter notebook called Code_Final_Assessment.ipynb. While it generally follows the structure outlined in the methodology, under each figure in the report, the code corresponding to its creation will be cited for ease of reproducibility and transparency.\n",
    "\n",
    "The notebook of the completed report will be accessible in my Github Repo as \"Report_Final_Assessment\". Furthermore, the data needed to reproduce these maps will also be uploaded to a google drive folder. It will also be made available in a folder called \"data\" uploaded to this repository.\n",
    "\n",
    "## [Google Drive Folder with Data](https://drive.google.com/drive/folders/1la5A6jrOylBtMjtsFkAid9vLym6hIm2G?usp=drive_link)\n",
    "## [My Github Repo](https://github.com/nb22-coder/210006819_UA_IRP)\n",
    "## <a href=\"Code_Final_Assessment.ipynb#-Reproducing-the-Code\">Notebook with the code</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2934a-607e-46f0-80c2-145c830a5d1f",
   "metadata": {},
   "source": [
    "### Libraries for Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9300e-d626-46cf-a5f9-abd9d74f2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!pip install folium\n",
    "import folium\n",
    "import shapely.geometry\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530d4294-999e-473a-b794-46518f7538ed",
   "metadata": {},
   "source": [
    "# 1. Vulnerability Data\n",
    "The data for displacement is sourced from the Indices of Multiple Deprivation dataset. The code below must combine the 2019 Indices for Multiple Deprivation with the geometries of the LSOAs (chosen granularity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba692de-ca42-467d-8b54-943a57889caf",
   "metadata": {},
   "source": [
    "### 1.1 LSOA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea980fed-5f28-40a3-b372-e61976b68665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMD 2019 File\n",
    "lsoa_geo = gpd.read_file(\"LSOA geometry/LSOA_2004_London_Low_Resolution.shp\")\n",
    "# I need to first find how many LSOAs are in London\n",
    "lsoa_geo[\"LSOA_CODE\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affd359-6bf1-46ae-94af-d1d9de91f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoa_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30782c3-e564-4f7b-9ca2-f8729c4ee0e2",
   "metadata": {},
   "source": [
    "### 1.2 Indices of Multiple Deprivation Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b4cff-a5ed-49f3-bd09-a81fc7bb0d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd =pd.read_csv(\"Vulnerability Index/cdrc_imd_data.csv\")\n",
    "\n",
    "#Show column headings\n",
    "imd.head()\n",
    "\n",
    "# I need to first find how the LSOAs look in this file\n",
    "imd[\"ls11cd\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced63ab2-38b3-4176-a054-89b57c6ac467",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef819a3f-5aa8-4cf2-b084-865750fcc540",
   "metadata": {},
   "source": [
    "### 1.3 Joining for Vulnerability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6104be-9c1c-4ae3-99ad-ae6fb15eb7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok so there is a discrepancy between the 'ls11cd' and the 'LSOA_CODE' but i need to initiate the merge nonetheless\n",
    "\n",
    "# Here I check if the LSOA codes are strings (important for merging later)\n",
    "lsoa_geo['LSOA_CODE'] = lsoa_geo['LSOA_CODE'].astype(str)\n",
    "imd['ls11cd'] = imd['ls11cd'].astype(str)\n",
    "\n",
    "# Now I initiate merge between the two so I can get the geo_data on the indices of multiple deprivation \n",
    "imd_merged = lsoa_geo.merge(imd, how='inner', left_on='LSOA_CODE', right_on='ls11cd')\n",
    "\n",
    "print(imd_merged.shape)\n",
    "print(imd_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c747d-aaba-4ef7-a27a-25cc839f5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unecessary columns found on the merge\n",
    "print(imd_merged.columns.tolist())\n",
    "\n",
    "columns_to_drop = ['MSOA_CODE', 'MSOA_NAME', 'STWARDCODE', 'STWARDNAME','ls11cd', 'la19nm', 'england_imd_rank', 'england_imd_decile']\n",
    "\n",
    "imd_merged = imd_merged.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55602a10-4eb2-4e4e-bb0f-46f974f5dfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imd_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1633915-c67a-4571-b4b6-f42863001c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I make sure there are not Nans\n",
    "print(imd_merged.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8900ad-17ea-4824-a7cc-b703e0d1f2bf",
   "metadata": {},
   "source": [
    "Now that I have conducted this merge I need to mirror the index so that 1= lowest vulnerability to displacement, to 10=highest vulnerability to displacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332ffbc-1647-44f6-8541-8fe8488e7223",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd_merged['london_imd_decile'] = 11 - imd_merged['london_imd_decile']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397af020-7a36-4f16-8f64-96deed0dee4f",
   "metadata": {},
   "source": [
    "# 2. Visualising Vulnerability Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179014a-1555-407c-8f13-94cf5eff96bc",
   "metadata": {},
   "source": [
    "## 2.1 Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144da63d-1b48-4dc0-a912-77f5f7e1fc35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by borough and describe IMD decile\n",
    "borough_dstats = imd_merged.groupby('LA_NAME')['london_imd_decile'].describe()\n",
    "\n",
    "borough_dstats = borough_dstats.sort_values('mean', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "print(borough_dstats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b1628-d44b-4a79-8086-16c2e2a6a019",
   "metadata": {},
   "source": [
    "## 2.2 Modelling Vulnerability Index\n",
    "This measure is much more straightforward for understanding vulnerability to displacement because it is a summative measure of multiple types of deprivation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5082f73-985d-4bc3-9e7f-1d70bcccac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3998d-798d-4a2e-8bb8-0921be41193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter most and least vulnerable LSOAs\n",
    "most_vulnerable = imd_merged[imd_merged['london_imd_decile'] == 10]\n",
    "least_vulnerable = imd_merged[imd_merged['london_imd_decile'] == 1]\n",
    "\n",
    "# Display the top rows of each group\n",
    "print(\"Most vulnerable LSOAs:\")\n",
    "print(most_vulnerable[['LSOA_CODE', 'LA_NAME', 'london_imd_decile']].head(10))\n",
    "\n",
    "print(\"\\nLeast vulnerable LSOAs:\")\n",
    "print(least_vulnerable[['LSOA_CODE', 'LA_NAME', 'london_imd_decile']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53e9d5-fa99-4d8a-bbc0-98cab37f7c5f",
   "metadata": {},
   "source": [
    "## 2.3 Chloropleth of Vulnerability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e79924-93aa-48b6-98f9-37c2372733f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am plotting total vulnerability, using IMD as a proxy for vulnerability to displacement and here I am using a chloropleth map to visualise this\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "imd_merged.plot(column='london_imd_decile',\n",
    "             cmap='Reds',\n",
    "             legend=True,\n",
    "             edgecolor='black',\n",
    "             linewidth=0.2,\n",
    "             ax=ax)\n",
    "\n",
    "ax.set_title('London Vulnerability (IMD) Deciles by LSOA', fontsize=16)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "# Darker red (higher score) = more deprived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a31b4-9a86-4f55-a126-f84666d4aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am segmenting the vulnerability decile by borough to represent the boroughs most vulnerable to displacement due to their relative deprivation.\n",
    "la_imd = imd_merged.groupby('LA_NAME')['london_imd_decile'].mean().sort_values()\n",
    "\n",
    "# Bar plot\n",
    "la_imd.plot(kind='barh', figsize=(10, 12), color='red')\n",
    "plt.title('Average Vulnerability (IMD) Decile by London Borough')\n",
    "plt.xlabel('Average Decile (Higher = More Vulnerable)')\n",
    "plt.ylabel('London Borough')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f0bc38-fccd-4929-9dfd-780d45868317",
   "metadata": {},
   "source": [
    "## 2.4 Supplementary Information: Barriers to Housing and Services Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f02cb9-fefb-4b4c-aa8e-237ad165eec8",
   "metadata": {},
   "source": [
    "While vulnerability is constituted by a myriad of factors, barriers to housing and services are essential for showing how housing informs the wider dimensions of vulnerability within areas of London. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15cbc75-b78d-447f-8b24-f2c927c489e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see boroughs like Richmond upon Thames, Kingston upon Thames, and Bromley have the lowest vulnerability scores, versus Tower Hamlets, Newham, Hackney, and Barking and Dagenham are all listed as the most vulnerable to displacement\n",
    "hsv_dstats = imd_merged.groupby('LA_NAME')['london_imd_decile'].describe()\n",
    "\n",
    "hsv_dstats = hsv_dstats.sort_values('mean')\n",
    "\n",
    "print(hsv_dstats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f2b51d-41eb-4252-99eb-2e59430450bf",
   "metadata": {},
   "source": [
    "## 2.5 Chloropleth of Housing and Services Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2bcee-bcab-46e1-8fe7-bb909066f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am subsetting to visualise housing and services vulnerability\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "imd_merged.plot(column='barriers_london_decile',\n",
    "             cmap='coolwarm',\n",
    "             legend=True,\n",
    "             edgecolor='black',\n",
    "             linewidth=0.2,\n",
    "             ax=ax)\n",
    "\n",
    "ax.set_title('London Housing and Services Vulnerability Deciles by LSOA', fontsize=16)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "# Red means more barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5da10d-dfb6-4c8b-ba56-d8c159205613",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here are the average housing and services vulnerability (ex. housing affordability, homelessness, household overcrowding)\n",
    "la_imd = imd_merged.groupby('LA_NAME')['barriers_london_decile'].mean().sort_values()\n",
    "\n",
    "# This bar plot shows the subsetted vulnerability just for housing and services access\n",
    "la_imd.plot(kind='barh', figsize=(10, 12), color='dodgerblue')\n",
    "plt.title('Average Housing and Services Vulnerability Decile by London Borough')\n",
    "plt.xlabel('Average Decile (Higher = More Vulnerable)')\n",
    "plt.ylabel('London Borough')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d32148-d336-4107-80e0-25061ff91924",
   "metadata": {},
   "source": [
    "## 2.6 Interactive Model of Displacement Vulnerability Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e87917-a817-43c2-a2c1-ad31cec90706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm setting the orientation of the base map\n",
    "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Now I'm projecting the imd_merged geodataframe onto the base map to show the \n",
    "folium.Choropleth(\n",
    "    geo_data=imd_merged,\n",
    "    data=imd_merged,\n",
    "    columns=['LSOA_CODE', 'london_imd_decile'],\n",
    "    key_on='feature.properties.LSOA_CODE',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='London IMD Decile',\n",
    "    highlight=True,\n",
    ").add_to(m)\n",
    "\n",
    "# Here is where I make the folium map clockable so you can explore the visualisation in greater detail\n",
    "style_function = lambda x: {\n",
    "    'fillColor': 'transparent',\n",
    "    'color': 'black',\n",
    "    'weight': 0.1,\n",
    "    'fillOpacity': 0\n",
    "}\n",
    "\n",
    "popup_fields = ['LSOA_CODE', 'LA_NAME', 'london_imd_decile', 'london_imd_rank', 'barriers_london_decile', 'barriers_london_rank']\n",
    "\n",
    "popup = folium.GeoJson(\n",
    "    imd_merged,\n",
    "    style_function=style_function,\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=popup_fields,\n",
    "        aliases=[\n",
    "            'LSOA Code:',\n",
    "            'Borough:',\n",
    "            'Vulnerability (IMD) Decile:',\n",
    "            'Vulnerability (IMD) Rank:',\n",
    "            'Barriers to Housing Vulnerability Decile:',\n",
    "            'Barriers to Housing Vulnerability Rank:'\n",
    "        ],\n",
    "        sticky=True\n",
    "    )\n",
    ")\n",
    "\n",
    "popup.add_to(m)\n",
    "\n",
    "# This is to work with the layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# I can export this file now \n",
    "m.save('london_imd_clickable_map.html')\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec6daa-ab16-4972-a969-fc53fa659d67",
   "metadata": {},
   "source": [
    "# 3. House Price Change (Neighborhood Change Factor 1)\n",
    "LSOA-level house price change and average weekly income change will be combined to reflect the socioeconomic shifts occurring in neighborhoods that are vulnerable to displacement from gentrification. \n",
    "## 3.1 Cleaning and Loading House Price Change Data (Neighborhood Change Factor 1)\n",
    "This data is sourced from the Land Registry but it only exists on a borough level not LSOA so we will need to simulate this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1302d-1786-4350-acbe-9def9fb6731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_index_LSOA =pd.read_csv(\"Neighborhood Change/land-registry-house-prices-LSOA.csv\")\n",
    "\n",
    "# Cleaning the index file\n",
    "hp_index_LSOA.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdea7bf-289e-46ea-9c9e-6ae04d380790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hp_index_LSOA.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51815934-7666-4c49-b27e-49aea13f6c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all empty columns and rows with Nan\n",
    "hp_index_LSOA.isnull().sum()\n",
    "columns_to_drop_hp = [ 'Unnamed: 46', 'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57']\n",
    "\n",
    "hp_index_LSOA = hp_index_LSOA.drop(columns=columns_to_drop_hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81544926-b83e-449e-bde1-b8640aac3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an awkward space between the titles of some of the variables so we need to remove these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4861f89-03b7-404d-9247-b3e54d61ccbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hp_index_LSOA.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c74703-34e5-4089-938c-9bf77db98799",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_index_LSOA.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac164ea-20a3-42e1-8d6c-d3d8efb3437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now all of them are removed and the dataset is properly cleaned\n",
    "hp_index_LSOA = hp_index_LSOA.dropna()\n",
    "hp_index_LSOA.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1499c-f1a8-4c05-8f6c-752bf364e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_index_LSOA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d04bd2-7a6f-4867-8818-0a0212723e8b",
   "metadata": {},
   "source": [
    "## 3.2 Projecting LSOAs onto House Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489210c2-58c4-430d-8ce9-e56cd4248235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ok so there is a discrepancy between the 'ls11cd' and the 'LSOA_CODE' but i need to initiate the merge nonetheless\n",
    "\n",
    "# Checking if the LSOA codes are strings so we can merge\n",
    "lsoa_geo['LSOA_CODE'] = lsoa_geo['LSOA_CODE'].astype(str)\n",
    "hp_index_LSOA['code'] = hp_index_LSOA['code'].astype(str)\n",
    "\n",
    "# Initiating the merge between the geometry from the LSOA data and the house price index data\n",
    "hp_merged = lsoa_geo.merge(hp_index_LSOA, how='inner', left_on='LSOA_CODE', right_on='code')\n",
    "\n",
    "print(hp_merged.shape)\n",
    "print(hp_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a69958-ab67-4d8a-bdb0-84c4047ce4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# REVISIT HERE - EDITS\n",
    "# Nearest Neighbor Interpolation for multiple columns\n",
    "def nearest_neighbor_interpolate(hp_merged, target_columns):\n",
    "    # Ensure the target columns are numeric and handle missing values\n",
    "    hp_merged[target_columns] = hp_merged[target_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Create a mask of where the data is missing for each column\n",
    "    missing_masks = {col: hp_merged[col].isna() for col in target_columns}\n",
    "\n",
    "    # Extract coordinates for each geometry (using centroids)\n",
    "    coords = np.array([geom.representative_point().coords[0] for geom in hp_merged.geometry])\n",
    "    \n",
    "    # Loop through rows with missing values\n",
    "    for idx, row in hp_merged.iterrows():\n",
    "        for col in target_columns:\n",
    "            if pd.isna(row[col]):  # If the value is missing\n",
    "                # Find the nearest neighbor based on the distance between centroids\n",
    "                distances = np.linalg.norm(coords - coords[idx], axis=1)  # Euclidean distance between centroids\n",
    "                nearest_idx = np.argmin(distances[~missing_masks[col]])  # Find the closest non-missing value for the column\n",
    "                hp_merged.at[idx, col] = hp_merged.iloc[nearest_idx][col]\n",
    "    \n",
    "    return hp_merged\n",
    "\n",
    "# Example usage\n",
    "# Assuming `hp_merged` has already been loaded\n",
    "\n",
    "# Perform Nearest Neighbor Interpolation on the 'YE_dec_2017' and 'YE_dec_2007' columns\n",
    "hp_merged = nearest_neighbor_interpolate(hp_merged, target_columns=['YE_ mar_2017', 'YE_ dec_2007'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e85de9-5693-4e05-9501-edefb5608274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing a start and an end date to model percentage growth\n",
    "start_col = 'YE_ mar_2007'\n",
    "end_col = 'YE_ dec_2017'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7d78b-3e04-474f-91c4-f7fe76c6d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the equation I need to make sure the variables can be manipulated as numbers\n",
    "hp_merged[start_col] = pd.to_numeric(hp_merged[start_col], errors='coerce')\n",
    "hp_merged[end_col] = pd.to_numeric(hp_merged[end_col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b424d-c0f6-4d76-9892-abfb7d59deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(hp_index_LSOA[[start_col, end_col]].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8515d5-d434-45b8-aba6-1b1f33e6c88b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hp_index_LSOA[[start_col, end_col]].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5a6ff0-594e-4b7c-b343-2c86fff1a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294acf6-0e7a-471e-b7de-1410cb215157",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_merged.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25645115-8447-4dc2-9dd2-534113af5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_columns = ['YE_ mar_2007','YE_ dec_2017']\n",
    "if hp_merged[check_columns].isnull().values.any():\n",
    "    print(\"There are missing values in the specified columns.\")\n",
    "\n",
    "\n",
    "    non_missing = hp_merged.dropna(subset=check_columns)\n",
    "    missing = hp_merged[hp_merged[check_columns].isnull().any(axis=1)]\n",
    "\n",
    "else:\n",
    "    print(\"No missing values in the specified columns.\")\n",
    "    non_missing = hp_merged \n",
    "    missing = hp_merged.iloc[0:0]\n",
    "\n",
    "print(f\"Non-missing rows: {len(non_missing)}\")\n",
    "print(f\"Missing rows: {len(missing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598cc75-0e02-4423-b16e-7d254956c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values in the target columns\n",
    "missing_values_after = hp_merged[['YE_ mar_2007', 'YE_ dec_2017']].isna().sum()\n",
    "\n",
    "print(\"Missing values after interpolation:\")\n",
    "print(missing_values_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc0339a-f38a-4831-b553-953efbaf7eb7",
   "metadata": {},
   "source": [
    "# 4. Solving for House Price AAGR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a4eba-a061-4f1d-a2a1-204eb7d60f85",
   "metadata": {},
   "source": [
    "## 4.1 Calculating House Price AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f9916-9387-4357-8edf-35fccf985631",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 2017 - 2007  # 10 years\n",
    "\n",
    "hp_merged['price_aagr'] = ((hp_merged[end_col] / hp_merged[start_col]) ** (1/years) - 1) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51143ca1-cbf2-4bf1-816d-fc288c5b451a",
   "metadata": {},
   "source": [
    "This data still needs to be normalized once I'm working with more values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb1282-4aba-489d-a628-1070fabb76f0",
   "metadata": {},
   "source": [
    "## 4.2 Modelling House Price AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b28ea-1b69-4242-84ce-a64bf07224f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AAGR distribution\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(hp_merged['price_aagr'], bins=30, kde=True, color='teal')\n",
    "plt.title('Distribution of Average Annual Growth Rate in House Prices (AAGR) Across LSOAs')\n",
    "plt.xlabel('AAGR (%)')\n",
    "plt.ylabel('Number of LSOAs')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23317b18-77e9-434e-a1f6-46a106174257",
   "metadata": {},
   "source": [
    "### 4.2.1 Bar Plot for House Price AAGR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c758e7-1653-4a94-ab40-e3f1822b6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping AAGR by the borough to model change in housing prices\n",
    "borough_aagr = hp_merged.groupby('LA_NAME')['price_aagr'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,12))\n",
    "borough_aagr.plot(kind='barh', color='teal')\n",
    "plt.title('Average Annual Growth Rate (AAGR) in Housing Prices by London Borough')\n",
    "plt.xlabel('Average AAGR (%)')\n",
    "plt.ylabel('Borough')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac782b2e-1f0e-4eb7-a723-8c108206d122",
   "metadata": {},
   "source": [
    "### 4.2.2 Scatterplot for House Price AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9295cd-a8a9-440a-ab1c-22c879d1d8a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using scatterplots to model outliers\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='LA_NAME', y='price_aagr', data=hp_merged)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Housing Prices AAGR Distribution Across LSOAs within Boroughs')\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('AAGR (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4f71e-a7d4-4db8-9969-afbac4e882fd",
   "metadata": {},
   "source": [
    "### 4.2.3 Line Chart of House Price AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0fa8b-a356-4837-a83d-849ef7196b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REVISIT HERE - EDITS\n",
    "# Modelling three subplots of the house price AAGR distribution\n",
    "# Filter only YE_* columns \n",
    "price_columns = [col for col in hp_merged.columns if col.startswith('YE_')]\n",
    "\n",
    "# Remove rows with ':' or non-numeric data\n",
    "hp_merged = hp_merged[~hp_merged[price_columns].apply(lambda row: row.astype(str).str.contains(':').any(), axis=1)]\n",
    "hp_merged[price_columns] = hp_merged[price_columns].apply(pd.to_numeric, errors='coerce')\n",
    "hp_merged = hp_merged.dropna(subset=price_columns)\n",
    "\n",
    "# Group by LA_NAME and calculate average prices\n",
    "hp_df_grouped = hp_merged.groupby('LA_NAME')[price_columns].mean()\n",
    "\n",
    "# Transpose for plotting (so columns are boroughs, rows are years)\n",
    "hp_df_grouped = hp_df_grouped.T\n",
    "hp_df_grouped.index = [col.replace('YE_', '') for col in hp_df_grouped.index]  # clean up x-axis labels\n",
    "\n",
    "# Rank boroughs by latest available year\n",
    "latest_prices = hp_df_grouped.iloc[-1]  # last row = most recent quarter\n",
    "top10_boroughs = latest_prices.sort_values(ascending=False).head(10).index\n",
    "bottom10_boroughs = latest_prices.sort_values(ascending=True).head(10).index\n",
    "\n",
    "# Prepare color palettes\n",
    "palette_top = sns.color_palette(\"husl\", 10)\n",
    "palette_bottom = sns.color_palette(\"husl\", 10)\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Top 10\n",
    "for i, borough in enumerate(top10_boroughs):\n",
    "    axs[0].plot(hp_df_grouped.index, hp_df_grouped[borough], label=borough, color=palette_top[i])\n",
    "axs[0].set_title(\"Top 10 Boroughs by Most Recent Average Housing Price\")\n",
    "axs[0].set_ylabel(\"Average Price (£)\")\n",
    "axs[1].set_xlabel(\"Year/Quarter\")\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "# Bottom 10\n",
    "for i, borough in enumerate(bottom10_boroughs):\n",
    "    axs[1].plot(hp_df_grouped.index, hp_df_grouped[borough], label=borough, color=palette_bottom[i])\n",
    "axs[1].set_title(\"Bottom 10 Boroughs by Most Recent Average Housing Price\")\n",
    "axs[1].set_xlabel(\"Year/Quarter\")\n",
    "axs[1].set_ylabel(\"Average Price (£)\")\n",
    "axs[1].legend(loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71358785-a8f9-4a78-8008-345701154781",
   "metadata": {},
   "source": [
    "## 4.3 Interactive Model of Neighborhood Change Factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244dc9c5-b507-4634-9e4d-510dbd929eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base map centered on London\n",
    "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Adding a House Price AAGR Choropleth\n",
    "folium.Choropleth(\n",
    "    geo_data=hp_merged,\n",
    "    data=hp_merged,\n",
    "    columns=['LSOA_CODE', 'price_aagr'],\n",
    "    key_on='feature.properties.LSOA_CODE',\n",
    "    fill_color='viridis_r', \n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average Annual House Price Growth Rate (%)',\n",
    "    highlight=True\n",
    ").add_to(m)\n",
    "\n",
    "# Creating a popup\n",
    "folium.GeoJson(\n",
    "    hp_merged,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'transparent',\n",
    "        'color': 'black',\n",
    "        'weight': 0.2,\n",
    "        'fillOpacity': 0\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['LSOA_NAME', 'price_aagr'],\n",
    "        aliases=['LSOA:', 'Annual Growth Rate (%)'],\n",
    "        localize=True\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Saving the map as an html\n",
    "m.save('lsoa_aagr_map.html')\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f55e5a-5c9c-463c-b691-baed5deee4a0",
   "metadata": {},
   "source": [
    "# 5. Weekly Income Change (Neighborhood Change Factor 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39e8df-3777-4360-9b47-6ca6bc073184",
   "metadata": {},
   "source": [
    "## 5.1 Cleaning and Uploading Weekly Earnings Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fb674-1918-4264-973f-0d3c161cc70a",
   "metadata": {},
   "source": [
    "The average weekly pay between the years 2007-17 is reflected in borough-level data. For this dataset we must project LSOA-level data for proper visualisation of neighborhood change as caused by increasing average weekly income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16d30d-23a4-4d03-9ab7-64d321e448c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the cleaned data\n",
    "earnings_con = pd.read_csv('Neighborhood Change/earnings-residence-borough.csv')\n",
    "\n",
    "# Checking the structure\n",
    "print(earnings_con.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ceb9a8-e695-4151-8555-992a10d9c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "imd_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de429af-4b2f-4c68-9ec8-6f397701274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stripping any spaces or notations that will confuse the running of the code\n",
    "imd_merged['LA_NAME'] = imd_merged['LA_NAME'].str.strip()\n",
    "earnings_con['area'] = earnings_con['area'].str.strip()\n",
    "\n",
    "# Merging the weekly average income figures on the vulnerability index\n",
    "merged_df = imd_merged.merge(earnings_con, left_on='LA_NAME', right_on='area', how='left')\n",
    "\n",
    "print(merged_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a48b2-8938-4fe2-9dd5-a312987ef58c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99419f9f-29d7-4f0a-8e8d-c5ad2e84b015",
   "metadata": {},
   "source": [
    "## 5.2 Projecting LSOAs onto Weekly Earnings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf813d9-aacd-497d-b6b8-4c98632176d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting weekly income per LSOA for each year\n",
    "for year in range(2007, 2018):\n",
    "    pay_col = f'pay_{year}'\n",
    "    est_col = f'estimated_income_{year}'\n",
    "    \n",
    "# Just in case the column is missing I have it so it will skip over it and not force the projection\n",
    "    if pay_col in merged_df.columns:\n",
    "        merged_df[est_col] = merged_df[pay_col] * merged_df['london_imd_decile']\n",
    "\n",
    "# Retaining the relevant columns\n",
    "income_cols = [col for col in merged_df.columns if col.startswith('estimated_income_')]\n",
    "output_cols = ['LSOA_CODE', 'LA_NAME', 'geometry','london_imd_rank', 'london_imd_decile'] + income_cols\n",
    "merged_df[output_cols].to_file('lsoa_estimated_weekly_income_2007_2017.geojson', driver='GeoJSON')\n",
    "\n",
    "print(\"Projected LSOA weekly pay for 2007–2017 saved to 'lsoa_estimated_weekly_income_2007_2017.geojson'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bc749-c888-433a-b4ef-daa413877584",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_earnings = gpd.read_file('lsoa_estimated_weekly_income_2007_2017.geojson')\n",
    "print(geo_earnings.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30449dc2-1074-4d5e-9d23-452d13e723ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of years between periods\n",
    "# years is already defined \n",
    "\n",
    "# Make sure earnings columns are numeric\n",
    "geo_earnings['estimated_income_2011'] = pd.to_numeric(geo_earnings['estimated_income_2011'], errors='coerce')\n",
    "geo_earnings['estimated_income_2017'] = pd.to_numeric(geo_earnings['estimated_income_2017'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37453dd1-564b-4e4e-998c-260e2b55f326",
   "metadata": {},
   "source": [
    "## 5.3 Calculating Average Weekly Income AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4afdbc-556c-4efd-b21b-36953c10c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating AAGR in average weekly incomes\n",
    "geo_earnings['earnings_aagr'] = ((geo_earnings['estimated_income_2017'] / geo_earnings['estimated_income_2011']) ** (1/years) - 1) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230b227-cba2-48c7-b382-5f5f1917cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see here how the projected incomes are reflected in their AAGR growth rate with these random LSOAs\n",
    "geo_earnings[['LSOA_CODE', 'estimated_income_2011', 'estimated_income_2017', 'earnings_aagr']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f98f5-e270-4375-b01c-3ec8d96e633d",
   "metadata": {},
   "source": [
    "## 5.3 Modelling Estimated AAGR in London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6384a-4da1-4ba6-82d2-5a254122b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am checking for valid geometries to ensure that this can be modelled geospatially\n",
    "earnings_gdf = gpd.GeoDataFrame(geo_earnings, geometry='geometry')\n",
    "earnings_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9000c-dea8-4158-a2e2-42dc4b1416b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d439a1-9bc2-4120-8c9a-02bbdf9dcc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf.set_crs(epsg=4326, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40fe28-eb8e-430e-9ef8-309fe4212b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf['geometry'] = hp_merged.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe86c6-afdd-4d7e-a58f-bd8b487114c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6b590-6786-4cbd-9c37-5f56b3d9f0f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here I am checking for valid geometries to ensure that this can be modelled geospatially\n",
    "earnings_gdf = gpd.GeoDataFrame(geo_earnings, geometry='geometry')\n",
    "\n",
    "# Transforming the CRS to EPSG: 27700 \n",
    "earnings_gdf = earnings_gdf.set_crs(epsg=4326, allow_override=True)\n",
    "\n",
    "earnings_gdf = earnings_gdf.to_crs(epsg=27700) \n",
    "\n",
    "earnings_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d74b8-81dc-453a-b2ee-7a466dceeb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf = earnings_gdf[earnings_gdf.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedc71a6-baf1-4f07-9c52-b4d44428dd17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(earnings_gdf.geom_type.value_counts())\n",
    "# Dtype is not correct so we need to designate the geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09ab5fb-6d79-4708-9c28-b67faab1ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf['geometry'] = earnings_gdf.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "earnings_gdf = gpd.GeoDataFrame(earnings_gdf, geometry='geometry', crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f37a4c-bb72-4c38-b0e5-4620eacdb445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_multipolygon(geometry):\n",
    "    if isinstance(geometry, Polygon):\n",
    "        return MultiPolygon([geometry])\n",
    "    return geometry\n",
    "\n",
    "earnings_gdf['geometry'] = earnings_gdf['geometry'].apply(ensure_multipolygon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cdcf0-90a5-4ab3-ad1c-b7d6350037dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am subsetting to visualise housing and services vulnerability\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "earnings_gdf.plot(column='earnings_aagr',\n",
    "             cmap='coolwarm',\n",
    "             legend=True,\n",
    "             edgecolor='black',\n",
    "             linewidth=0.2,\n",
    "             ax=ax)\n",
    "\n",
    "ax.set_title('London Housing and Services Vulnerability Deciles by LSOA', fontsize=16)\n",
    "ax.axis('off')\n",
    "plt.show()\n",
    "# Red means more barriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c002a-fba3-49ec-86b7-b3399429f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(earnings_gdf.geometry.is_empty.sum())   # How many are empty?\n",
    "print(earnings_gdf.geometry.notnull().sum())  # How many are non-null?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d38092-ad2d-4bd3-91f0-d63a0c3047c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf = earnings_gdf[earnings_gdf.geometry.notnull()]\n",
    "earnings_gdf = earnings_gdf[~earnings_gdf.geometry.is_empty]\n",
    "earnings_gdf = earnings_gdf[earnings_gdf.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22361d-0bc5-4389-ae81-aeabf6e30d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37944bb-67a0-4260-9c59-05b9a648c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_gdf.set_crs(epsg=27700, inplace=True)  # Only if not already set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb3131-11bb-4a8f-bdf3-0a9ed546e9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the choropleth map\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "earnings_gdf.plot(column=\"earnings_aagr\", ax=ax, legend=True,\n",
    "         legend_kwds={'label': \"Average Annual Earnings Growth Rate (%)\",\n",
    "                      'orientation': \"horizontal\"})\n",
    "plt.title(\"Earnings AAGR Across London\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eaf24c-3f29-4356-adda-51e51b4004fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(earnings_gdf.head())\n",
    "print(earnings_gdf.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8543c71-7cfc-4ff4-9597-768226d4344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting AAGR distribution to see how we need to calculate the composite vulnerability score\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(earnings_gdf['earnings_aagr'], bins=30, kde=True, color='purple')\n",
    "plt.title('Distribution of Average Annual Growth Rate (AAGR) for Weekly Income Across LSOAs')\n",
    "plt.xlabel('AAGR (%)')\n",
    "plt.ylabel('Number of LSOAs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f591a-fed1-4424-85f9-291c4e35962d",
   "metadata": {},
   "source": [
    "### 5.3.2 Bar Plot for Weekly Income AAGR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc949b9-2b33-4702-af5e-8bd0c4cbef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usec groupby function to sort the AAGR by London borough\n",
    "borough_aagr = earnings_gdf.groupby('LA_NAME')['earnings_aagr'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,12))\n",
    "borough_aagr.plot(kind='barh', color='purple')\n",
    "plt.title('Average Annual Growth Rate (AAGR) in Weekly Income by London Borough')\n",
    "plt.xlabel('Average AAGR (%)')\n",
    "plt.ylabel('Borough')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246d54e-9c27-4cc8-bb75-432db0dbf343",
   "metadata": {},
   "source": [
    "### 5.3.3 Line Chart of Weekly Income AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e7026-565a-48c5-a42d-3e73415081c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using an if and for function to isolate the columns that have to do with average weekly earnings\n",
    "income_columns = [col for col in geo_earnings.columns if col.startswith('estimated_income_20')]\n",
    "\n",
    "\n",
    "# Producing a copy so the cleaned segment doesnt interfere with existing geo_earnings\n",
    "earnings_cleaned = geo_earnings.copy()\n",
    "\n",
    "# Instructing the system to ignore the columns that contain colons to denote a lack of input\n",
    "earnings_cleaned = earnings_cleaned[~earnings_cleaned[income_columns].apply(lambda row: row.astype(str).str.contains(':').any(), axis=1)]\n",
    "\n",
    "# Now we're converting to a float64 so it can be adapted to a line chart\n",
    "earnings_cleaned[income_columns] = earnings_cleaned[income_columns].apply(pd.to_numeric, errors='coerce')\n",
    "earnings_cleaned = earnings_cleaned.dropna(subset=income_columns)\n",
    "\n",
    "# Here is the grouping by boroughs\n",
    "earnings_grouped = earnings_cleaned.groupby('LA_NAME')[income_columns].mean()\n",
    "\n",
    "# Finding the mean of each borough\n",
    "earnings_grouped = geo_earnings.groupby('LA_NAME')[income_columns].mean()\n",
    "\n",
    "# Transpose for plotting (so columns are boroughs, rows are years)\n",
    "earnings_grouped = earnings_grouped.T\n",
    "earnings_grouped.index = [col.replace('estimated_income_', '') for col in earnings_grouped.index]\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "for borough in earnings_grouped.columns:\n",
    "    plt.plot(earnings_grouped.index, earnings_grouped[borough], label=borough)\n",
    "\n",
    "plt.title(\"Average Projected Weekly Earnings by Borough Over Time\")\n",
    "plt.xlabel(\"Year/Quarter\")\n",
    "plt.ylabel(\"Average Weekly Income (£)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2355752-f65e-4a3e-b8b1-4a8c01e0fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting for the income columns\n",
    "income_columns = [col for col in geo_earnings.columns if col.startswith('estimated_income_20')]\n",
    "\n",
    "# Cleaning the data\n",
    "earnings_cleaned = geo_earnings.copy()\n",
    "earnings_cleaned = earnings_cleaned[~earnings_cleaned[income_columns].apply(lambda row: row.astype(str).str.contains(':').any(), axis=1)]\n",
    "earnings_cleaned[income_columns] = earnings_cleaned[income_columns].apply(pd.to_numeric, errors='coerce')\n",
    "earnings_cleaned = earnings_cleaned.dropna(subset=income_columns)\n",
    "\n",
    "# Step 3: Group by borough and calculate average earnings\n",
    "earnings_grouped = earnings_cleaned.groupby('LA_NAME')[income_columns].mean()\n",
    "\n",
    "# Step 4: Transpose for plotting\n",
    "earnings_grouped = earnings_grouped.T\n",
    "earnings_grouped.index = [col.replace('estimated_income_', '') for col in earnings_grouped.index]\n",
    "\n",
    "# Step 5: Rank boroughs by latest year's earnings\n",
    "latest_earnings = earnings_grouped.iloc[-1]\n",
    "top10 = latest_earnings.sort_values(ascending=False).head(10).index\n",
    "bottom10 = latest_earnings.sort_values().head(10).index\n",
    "\n",
    "# Step 6: Plot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Top 10\n",
    "palette_top = sns.color_palette(\"husl\", 10)\n",
    "for i, borough in enumerate(top10):\n",
    "    axs[0].plot(earnings_grouped.index, earnings_grouped[borough], label=borough, color=palette_top[i])\n",
    "axs[0].set_title(\"Top 10 Boroughs by Latest Average Earnings\")\n",
    "axs[0].set_ylabel(\"Average Earnings (£)\")\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "# Bottom 10\n",
    "palette_bottom = sns.color_palette(\"husl\", 10)\n",
    "for i, borough in enumerate(bottom10):\n",
    "    axs[1].plot(earnings_grouped.index, earnings_grouped[borough], label=borough, color=palette_bottom[i])\n",
    "axs[1].set_title(\"Bottom 10 Boroughs by Latest Average Earnings\")\n",
    "axs[1].set_xlabel(\"Year\")\n",
    "axs[1].set_ylabel(\"Average Earnings (£)\")\n",
    "axs[1].legend(loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73bc85-02b6-4941-b9b8-ab1c08c496ae",
   "metadata": {},
   "source": [
    "## 5.4 Interactive Model of Weekly Income AAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c2a2a1-690d-4222-a4c8-84e24f44ad5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base map centered on London\n",
    "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Adding an Earnings AAGR Choropleth\n",
    "folium.Choropleth(\n",
    "    geo_data=earnings_gdf,\n",
    "    data=earnings_gdf,\n",
    "    columns=['LSOA_CODE', 'earnings_aagr'],\n",
    "    key_on='feature.properties.LSOA_CODE',\n",
    "    fill_color='viridis_r', \n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Average Annual Earnings Growth Rate (%)',\n",
    "    highlight=True\n",
    ").add_to(m)\n",
    "\n",
    "# Creating a popup\n",
    "folium.GeoJson(\n",
    "    earnings_gdf,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'transparent',\n",
    "        'color': 'black',\n",
    "        'weight': 0.2,\n",
    "        'fillOpacity': 0\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['LA_NAME', 'earnings_aagr'],\n",
    "        aliases=['LSOA:', 'Annual Growth Rate (%)'],\n",
    "        localize=True\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Saving the map as an html\n",
    "m.save('earnings_aagr_map.html')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45a479-bdd7-4611-b555-411f62d253ad",
   "metadata": {},
   "source": [
    "# 6. Computing a Neighborhood Change Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c70a5-962d-4269-b9ab-00531f6a4da8",
   "metadata": {},
   "source": [
    "## 6.1 Combining into the Neighborhood Change indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5790d-69cf-4719-af36-1714b012d198",
   "metadata": {},
   "source": [
    "Now is where I need to normalise indicators using (min-max or z-score), I am electing to normalise using min-max because the earnings AAGR is not normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e027c-70cf-485b-892d-25b0e9dfda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I need to merge my two indicators into a single dataframe, for ease of processing I can cut out quite a few columns to make it as straightforward as possible\n",
    "neighborhood_gdf = earnings_gdf.merge(hp_merged, on='LSOA_CODE', how='inner')\n",
    "neighborhood_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7366ef5-6ec2-47e1-8a42-0abe1a7a9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neighborhood_gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd01d6b4-dbf7-442c-b1b1-08d371752372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the attributes we need, we dont need the total for now.\n",
    "keep_cols= [\n",
    "    'LSOA_CODE',\n",
    "     'LA_NAME_x',\n",
    "    'geometry_x',\n",
    "     'london_imd_rank',\n",
    "     'london_imd_decile',\n",
    "     'earnings_aagr',\n",
    "     'price_aagr'\n",
    "]\n",
    "\n",
    "neighborhood_gdf = neighborhood_gdf[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b042e-04e6-4799-8cf2-7ab750b47dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94002881-0be5-4123-9bf5-775b49469ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling both variables from 0 to 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "scaled = scaler.fit_transform(neighborhood_gdf[['earnings_aagr', 'price_aagr']])\n",
    "\n",
    "neighborhood_gdf['earnings_change_scaled'] = scaled[:, 0]\n",
    "neighborhood_gdf['price_change_scaled'] = scaled[:, 1]\n",
    "\n",
    "# Creating the change index (equal weighting since both influence displacement)\n",
    "neighborhood_gdf['nb_change_index'] = (\n",
    "    0.5 * neighborhood_gdf['earnings_change_scaled'] +\n",
    "    0.5 * neighborhood_gdf['price_change_scaled']\n",
    ")\n",
    "neighborhood_gdf.rename(columns={'london_imd_decile': 'vuln_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b806a17-d738-44ec-a0f5-c6696e31a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_gdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcbe258-ced7-406c-a7a3-b8c18ac0f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'nb_change_index' is NaN\n",
    "neighborhood_gdf = neighborhood_gdf.dropna(subset=['nb_change_index'])\n",
    "\n",
    "print(neighborhood_gdf.isnull().values.any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579cd10-0c4b-45a1-9dbe-47d8da260cb2",
   "metadata": {},
   "source": [
    "## 6.2 Visualising Neighborhood Change Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb854e0-bd1b-4c76-9803-bb4150b497e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf = gpd.GeoDataFrame(neighborhood_gdf, geometry='geometry_x')\n",
    "\n",
    "# Assigning a CRS to ensure accurate projection\n",
    "nb_gdf = nb_gdf.set_crs(epsg=27700, allow_override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e50aa1-6f90-477a-ad05-5a625042bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'LSOA_CODE' is a string that can be manipulated\n",
    "nb_gdf['LSOA_CODE'] = nb_gdf['LSOA_CODE'].astype(str)\n",
    "\n",
    "# Creating the base map that is centered on London\n",
    "nb_map = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Adding choropleth layer with gradient\n",
    "folium.Choropleth(\n",
    "    geo_data=nb_gdf,\n",
    "    name='Neighborhood Change Index',\n",
    "    data=nb_gdf,\n",
    "    columns=['LSOA_CODE', 'nb_change_index'],\n",
    "    key_on='feature.properties.LSOA_CODE',\n",
    "    fill_color='viridis',  # or try 'PuBuGn', 'Viridis', 'OrRd'\n",
    "    fill_opacity=0.8,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Neighborhood Change Index',\n",
    "    highlight=True\n",
    ").add_to(nb_map)\n",
    "\n",
    "# Here I am adding a tooltip so you can hover over the individual LSOAs\n",
    "tooltip = folium.GeoJson(\n",
    "    nb_gdf,\n",
    "    style_function=lambda x: {'fillOpacity': 0, 'weight': 0.3},\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['LSOA_CODE', 'LA_NAME_x', 'nb_change_index'],\n",
    "        aliases=['LSOA Code:', 'Borough:', 'Change Index:'],\n",
    "        sticky=True\n",
    "    )\n",
    ")\n",
    "tooltip.add_to(nb_map)\n",
    "\n",
    "# Here is where I can control\n",
    "folium.LayerControl().add_to(nb_map)\n",
    "\n",
    "# Saving the file\n",
    "nb_map.save('london_nbchange_gradient.html')\n",
    "nb_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e110a9-c0d1-4737-98a2-48e44c3f0679",
   "metadata": {},
   "source": [
    "# 7. Composite Displacement Vulnerability Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea2f92-5cad-4e71-8f8f-8782bd75fe63",
   "metadata": {},
   "source": [
    "Now is the time to calculate a composite displacement vulnerability index using our two factors: \n",
    "- vulnerability index (e.g., based on IMD)\n",
    "- neighborhood change index (e.g., based on AAGR of earnings and housing prices)\n",
    "\n",
    "These should both now be on a 0–10 scale, with higher = more at risk/change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569117c-9ed7-449f-a249-d174de669c84",
   "metadata": {},
   "source": [
    "Map the Overlap and Tensions\n",
    "Key variables for spatial visualization:\n",
    "- vulnerability_index\n",
    "- neighborhood_change_index\n",
    "- displacement_risk_scaled\n",
    "\n",
    "Now we can map a bivariate chloropleth to show distribution of vulnerability and neighborhood change\n",
    "\n",
    "Here composite vulnerability can be visualised using:\n",
    "\n",
    "- High vulnerability + High change → Most at risk\n",
    "\n",
    "- High vulnerability + Low change → Stable but vulnerable\n",
    "\n",
    "- Low vulnerability + High change → Changing but resilient\n",
    "\n",
    "- Low vulnerability + Low change → Low risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea969c9a-8b60-4518-a980-e6cc5179e78a",
   "metadata": {},
   "source": [
    "## 7.1 Defining Composite Displacement Risk Score (CDRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0b020-1f41-42c5-8e34-884ee5ffb467",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "nb_gdf.rename(columns={'london_imd_decile': 'vuln_index'}, inplace=True)\n",
    "\n",
    "\n",
    "nb_gdf[['vuln_scaled', 'change_scaled']] = scaler.fit_transform(nb_gdf[['vuln_index', 'nb_change_index']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209a143-707e-43b2-be83-90d2c184bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['displacement_risk'] = (nb_gdf['vuln_scaled'] + nb_gdf['change_scaled']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9535ef-cc9b-4247-bb03-b148ecb61c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['displacement_risk_10'] = nb_gdf['displacement_risk'] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efd90b8-26d7-4bb4-90d2-52fe22a0c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb_gdf[['displacement_risk', 'vuln_scaled', 'change_scaled']].corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8d2f40-b8a1-4487-aed0-72dd2927d0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Overlap/Tension Categories\n",
    "def categorize(val):\n",
    "    if val < 0.33:\n",
    "        return 'Low'\n",
    "    elif val < 0.66:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "\n",
    "nb_gdf['vuln_cat'] = nb_gdf['vuln_index'].apply(categorize)\n",
    "nb_gdf['change_cat'] = nb_gdf['nb_change_index'].apply(categorize)\n",
    "\n",
    "# Combining for a bivariate category where both can be visualised\n",
    "nb_gdf['bivariate_class'] = nb_gdf['vuln_cat'] + '-' + nb_gdf['change_cat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac4939-74e8-4192-89ff-8e2af20bf88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['vuln_cat'] = pd.qcut(nb_gdf['vuln_scaled'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "nb_gdf['change_cat'] = pd.qcut(nb_gdf['change_scaled'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Combining to make a bivariate class\n",
    "nb_gdf['bivariate_class'] = nb_gdf['vuln_cat'].astype(str) + '-' + nb_gdf['change_cat'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e01c8a-9931-4936-af41-46473988da02",
   "metadata": {},
   "source": [
    "## 7.2 Descriptive Statistics for Combined Displacement Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1e570-ef22-4a26-aee6-634ca341c584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here I need to compute the Combined Displacement Risk Score\n",
    "nb_gdf['displacement_risk'] = nb_gdf['vuln_index'] + nb_gdf['nb_change_index']\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "nb_gdf['displacement_risk_scaled'] = scaler.fit_transform(nb_gdf[['displacement_risk']])\n",
    "\n",
    "\n",
    "# Grouping by borough and describing displacement index\n",
    "displaced_dstats = nb_gdf.groupby('LA_NAME_x')['displacement_risk_scaled'].describe()\n",
    "\n",
    "displaced_dstats = displaced_dstats.sort_values('mean', ascending=False)\n",
    "\n",
    "# Showing the result\n",
    "print(displaced_dstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69174194-1683-4c7f-9c94-38ff1e033779",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['displacement_risk'].hist(bins=20)\n",
    "plt.title('Displacement Index Distribution')\n",
    "plt.xlabel('Index Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Looking at the displacement index distribution\n",
    "print(nb_gdf[['displacement_risk', 'vuln_index', 'nb_change_index']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a145e52-542b-46f5-81bd-7ba19465b44b",
   "metadata": {},
   "source": [
    "Here we can tell that displacement_risk is highly correlated with vuln_index (0.91). Nb_change_index has low correlation with displacement_risk (0.22), likely because of smaller value ranges. Vuln_index and nb_change_index are weakly negatively correlated (-0.22) indicating that areas more vulnerable (increased deprivation) tend to show slightly less neighborhood change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd1f05-12d3-43e4-b153-591adf0c9470",
   "metadata": {},
   "source": [
    "## 7.3 Designating Bivariate Composite Risk Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeffa4d-a3a0-4ecc-abcb-b69617dd7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['vuln_cat'] = pd.qcut(nb_gdf['vuln_scaled'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "nb_gdf['change_cat'] = pd.qcut(nb_gdf['change_scaled'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Combine to here to make a bivariate class\n",
    "nb_gdf['bivariate_class'] = nb_gdf['vuln_cat'].astype(str) + '-' + nb_gdf['change_cat'].astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66cbfb-bfab-49cf-b5a7-e6ff1dd523d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb_gdf['bivariate_class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ea0cb-5c44-4698-8121-dae654227143",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gdf['displacement_risk'].hist(bins=20)\n",
    "plt.title('Displacement Index Distribution')\n",
    "plt.xlabel('Index Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f9a054-0118-4ea1-8132-8eb74b596b07",
   "metadata": {},
   "source": [
    "## 7.4 Spatial Visualisation of Composite Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cae39-fa77-4327-8248-c4ff4d384802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is where I designate the colors of the overlap bivariate map, its not just as straightforward as a normal chloropleth because of the bivariate classification\n",
    "bivariate_colors = {\n",
    "    'Low-Low': '#e8e8e8',\n",
    "    'Medium-Low': '#ace4e4',\n",
    "    'High-Low': '#5ac8c8',\n",
    "    'Low-Medium': '#dfb0d6',\n",
    "    'Medium-Medium': '#a5add3',\n",
    "    'High-Medium': '#5698b9',\n",
    "    'Low-High': '#be64ac',\n",
    "    'Medium-High': '#8c62aa',\n",
    "    'High-High': '#3b4994',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a637972e-5d5d-4c60-8865-51267fe8df85",
   "metadata": {},
   "source": [
    "To find these color classifications I consulted the [hexadecimal color codes](https://htmlcolorcodes.com/) and used their random color generator to designate these bivariate classes. It took some trial and error to find colors which showed the bivariate nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71566-4287-4e7a-8b46-5513fe315dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Inputting the bivariate choloropleth\n",
    "folium.GeoJson(\n",
    "    nb_gdf,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': bivariate_colors.get(feature['properties']['bivariate_class'], 'gray'),\n",
    "        'color': 'black',\n",
    "        'weight': 0.1,\n",
    "        'fillOpacity': 0.8,\n",
    "    },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['LSOA_CODE', 'LA_NAME_x', 'bivariate_class'],\n",
    "        aliases=['LSOA Code', 'Borough', 'Overlap Category'],\n",
    "    )\n",
    ").add_to(dis_m)\n",
    "\n",
    "dis_m.save('london_overlap_tensions_map.html')\n",
    "dis_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd21701-c423-4d2a-864b-4cd9f495e07c",
   "metadata": {},
   "source": [
    "# 8. Geodemographic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc839c-d176-48b1-bc6e-587d7969cb69",
   "metadata": {},
   "source": [
    "## 8.1 Collating and Cleaning the Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87eb96-1846-4764-8d7e-a9237a25c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell DOES NOT need to be run as there is already a file called merged_census_data.csv \n",
    "# I have included the code to show what needs to be done to combine the census layers\n",
    "# Here the CENSUS data is merged to itself as its split into 6 separate distinctions\n",
    "\n",
    "\n",
    "csv_directory = \"census_raw_data\"\n",
    "\n",
    "# We need a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(csv_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "# An empty DataFrame to store the merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(csv_directory, csv_file) # We create a consistent path\n",
    "    df_csv = pd.read_csv(csv_path, low_memory=False) #read each file\n",
    "    # Concatenate/Merge all columns, there is a pitfall here, you will get a duplicate oa_code from all csv files.\n",
    "    merged_data = pd.concat([merged_data, df_csv], axis=1)\n",
    "\n",
    "# Remove duplicate columns (keep the first occurrence)\n",
    "merged_data = merged_data.loc[:, ~merged_data.columns.duplicated()]\n",
    "\n",
    "# Save the merged dataset. You might want to do some pre-processing.\n",
    "merged_data.to_csv(\"census_raw_data/merged_census_data.csv\", index=False)\n",
    "# Be aware of the mixted dtype you are importing we unfortunatly have to deal with that later.\n",
    "# eventually you can avoid this to define the dtype on import method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac6548-d56f-4286-b433-2acd18d5465b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(merged_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4c631-f6d4-4ca5-a11e-a0977ab7d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I define  the list of 32 London boroughs\n",
    "london_boroughs = [\n",
    "    'Barking and Dagenham', 'Barnet', 'Bexley', 'Brent', 'Bromley', 'Camden',\n",
    "    'Croydon', 'Ealing', 'Enfield', 'Greenwich', 'Hackney', 'Hammersmith and Fulham',\n",
    "    'Haringey', 'Harrow', 'Havering', 'Hillingdon', 'Hounslow', 'Islington',\n",
    "    'Kensington and Chelsea', 'Kingston upon Thames', 'Lambeth', 'Lewisham',\n",
    "    'Merton', 'Newham', 'Redbridge', 'Richmond upon Thames', 'Southwark',\n",
    "    'Sutton', 'Tower Hamlets', 'Waltham Forest', 'Wandsworth', 'Westminster'\n",
    "]\n",
    "\n",
    "london_boroughs_clean = [b.lower().strip() for b in london_boroughs] #This line of code is stripping the columns to be sure they all match\n",
    "\n",
    "# Here we've got to identify the column that contains the area names — assuming it's the first one\n",
    "area_col = merged_data.columns[0]\n",
    "\n",
    "# Add a helper column for matching\n",
    "merged_data['_area_clean'] = merged_data[area_col].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Filtering for rows where the area name starts with a London borough\n",
    "filtered_data = merged_data[merged_data['_area_clean'].apply(\n",
    "    lambda x: any(x.startswith(b) for b in london_boroughs_clean)\n",
    ")]\n",
    "\n",
    "# Dropping the helper column that we used\n",
    "filtered_data.drop(columns=['_area_clean'], inplace=True)\n",
    "\n",
    "# Saving and storing the result for reproducibility\n",
    "filtered_data.to_csv(\"census_raw_data/london_census_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d56052-8dea-467d-a3ab-f79bae041dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08dcb2-aea9-4f4d-b23f-71544e15dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are making sure the LSOAs are stripped and sorted as strings \n",
    "filtered_data['2021 super output area - lower layer'] = filtered_data['2021 super output area - lower layer'].astype(str).str.strip()\n",
    "nb_gdf['LSOA_NAME_x'] = nb_gdf['LSOA_NAME_x'].astype(str).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60490a29-ea75-4951-8b93-f3ac1bae2556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is where we perform the merge between the\n",
    "census_merged_gdf = nb_gdf.merge(\n",
    "    filtered_data,\n",
    "    left_on='LSOA_CODE',\n",
    "    right_on='mnemonic',\n",
    "    how='inner'  # Only keeping the rows that match the other dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66398bb-be9c-4263-a19c-7ecae0f222e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rows with missing census data after merge:\",\n",
    "      census_merged_gdf['Total'].isna().sum()) # Here I am checking that all rows are filled and properly assigned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a144035-e28b-4fda-b374-06ed3767bc0a",
   "metadata": {},
   "source": [
    "## 8.2 Selecting Desired Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b2810-0039-4e3a-871a-ffb0e44adae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(census_merged_gdf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398adb10-da10-4819-8a01-0d10c17a632b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    'LSOA_CODE',\n",
    "    'LSOA_NAME_x', \n",
    "    'LA_CODE_x', \n",
    "    'LA_NAME_x', \n",
    "    'geometry_x', \n",
    "    'london_imd_rank', \n",
    "    'vuln_index',  \n",
    "    'earnings_aagr', \n",
    "    'price_aagr', 'earnings_change_scaled',\n",
    "    'price_change_scaled', \n",
    "    'nb_change_index', \n",
    "    'displacement_risk', \n",
    "    'displacement_risk_scaled', \n",
    "    'vuln_cat', \n",
    "    'change_cat', \n",
    "    'bivariate_class', \n",
    "    'Total', \n",
    "    'Aged 4 years and under', \n",
    "    'Aged 5 to 9 years', \n",
    "    'Aged 10 to 15 years', \n",
    "    'Aged 16 to 19 years', \n",
    "    'Aged 20 to 24 years', \n",
    "    'Aged 25 to 34 years', \n",
    "    'Aged 35 to 49 years', \n",
    "    'Aged 50 to 64 years', \n",
    "    'Aged 65 to 74 years', \n",
    "    'Aged 75 to 84 years', \n",
    "    'Aged 85 years and over', \n",
    "    'Total: All usual residents aged 16 years and over', \n",
    "    'No qualifications', 'Level 1 and entry level qualifications', 'Level 2 qualifications', 'Apprenticeship', 'Level 3 qualifications', 'Level 4 qualifications or above', 'Other qualifications', 'Total: All usual residents', 'Asian, Asian British or Asian Welsh', 'Black, Black British, Black Welsh, Caribbean or African', 'Mixed or Multiple ethnic groups', 'White', 'Other ethnic group', 'One-person household', 'Single family household', 'Other household types', 'Born in the UK', '10 years or more', '5 years or more, but less than 10 years', '2 years or more, but less than 5 years', 'Less than 2 years', 'Total: All households', 'Owned', 'Shared ownership', 'Social rented', 'Private rented', 'Lives rent free', 'Owns with a mortgage or loan or shared ownership', 'Private rented or lives rent free', 'Total: All usual residents.1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3590e4-23cb-4fe3-b70e-b416f80c1a4c",
   "metadata": {},
   "source": [
    "**SELECTED CENSUS FACTORS**\n",
    "\n",
    "| Variable Group      | Columns                                                             | Significance                                                           |\n",
    "|---------------------|------------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Age**             | Aged 25 to 34 years, Aged 65 to 74 years, Aged 75 to 84 years, Aged 85 years and over | Identifies young vs aging populations                              |\n",
    "| **Tenure**          | Private rented, Social rented, Owned, Owns with a mortgage or loan or shared ownership, Lives rent free | Tenure security and economic status                                  |\n",
    "| **Education**       | Level 4 qualifications or above, No qualifications                | Proxies for economic mobility and professional status                |\n",
    "| **Ethnicity**       | White, Black, Black British, Black Welsh, Caribbean or African, Asian, Asian British or Asian Welsh | Can help interpret patterns in demographic distribution                                        |\n",
    "| **Household Type**  | One-person household, Single family household                     | Useful to infer life stage or vulnerability                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f36b1-8cd6-49bb-9bad-b056b57e175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'Aged 25 to 34 years', 'Aged 65 to 74 years', 'Aged 75 to 84 years', 'Aged 85 years and over',\n",
    "    'Private rented', 'Social rented', 'Owned', 'Owns with a mortgage or loan or shared ownership', 'Lives rent free',\n",
    "    'Level 4 qualifications or above', 'No qualifications',\n",
    "    'White', 'Black, Black British, Black Welsh, Caribbean or African', 'Asian, Asian British or Asian Welsh',\n",
    "    'One-person household', 'Single family household',\n",
    "    'Less than 2 years', '10 years or more'\n",
    "]\n",
    "\n",
    "X = census_merged_gdf[selected_features].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6afbe-7b30-4f61-9637-fe6c6bc296be",
   "metadata": {},
   "source": [
    "## 8.3 Examining Correlations of Census Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1668ca-7bd6-4796-a0df-e06dbd3f1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used the scaler to put all these in percentages so now we can just run the correlation matrix\n",
    "selected_features = [\n",
    "    'Aged 25 to 34 years', 'Aged 65 to 74 years', 'Aged 75 to 84 years', 'Aged 85 years and over',\n",
    "    'Private rented', 'Social rented', 'Owned', 'Owns with a mortgage or loan or shared ownership', 'Lives rent free',\n",
    "    'Level 4 qualifications or above', 'No qualifications',\n",
    "    'White', 'Black, Black British, Black Welsh, Caribbean or African', 'Asian, Asian British or Asian Welsh',\n",
    "    'One-person household', 'Single family household',\n",
    "    'Less than 2 years', '10 years or more'\n",
    "]\n",
    "\n",
    "# Extracting and cleaning empty columns\n",
    "X = census_merged_gdf[selected_features].fillna(0)\n",
    "\n",
    "# Computing a correlation matrix\n",
    "corr_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', vmin=-1, vmax=1, square=True)\n",
    "plt.title(\"Correlation Matrix of Selected Census Features\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# This shows all the variables but is hard to interpret so we need to focus in on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7fc89-3aa0-4750-8f8a-6c98044e8f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation matrix\n",
    "corr = X.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "# Defining the threshold\n",
    "threshold = 0.75\n",
    "highly_correlated = (corr.abs() > threshold) & (corr.abs() < 1.0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(highly_correlated, cmap='coolwarm', cbar=False, annot=True)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.title('Highly Correlated Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b07d5f-6f59-4175-b849-ad7e5c0289d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'Aged 25 to 34 years', 'Aged 65 to 74 years', 'Aged 75 to 84 years', 'Aged 85 years and over',\n",
    "    'Private rented', 'Social rented', 'Owned', 'Owns with a mortgage or loan or shared ownership', 'Lives rent free',\n",
    "    'Level 4 qualifications or above', 'No qualifications',\n",
    "    'White', 'Black, Black British, Black Welsh, Caribbean or African', 'Asian, Asian British or Asian Welsh',\n",
    "    'One-person household',\n",
    "    'Less than 2 years', '10 years or more'\n",
    "]\n",
    "\n",
    "X = census_merged_gdf[selected_features].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6078016-48f6-46fb-8af1-583633ce13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets check again....\n",
    "corr_2 = X.corr()\n",
    "corr_2.style.background_gradient(cmap='coolwarm')\n",
    "\n",
    "threshold = 0.8\n",
    "highly_correlated_2 = (corr_2.abs() > threshold) & (corr_2.abs() < 1.0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(highly_correlated_2, cmap='coolwarm', cbar=False, annot=True)\n",
    "\n",
    "plt.title('New Highly Correlated Variables')\n",
    "plt.show()\n",
    "\n",
    "#Wahoo!!!, but before running the next part of the process, we also need to get ride of the NaN values. What a nightmare I know!."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34036045-a537-4c7f-9a11-0efd2286e13e",
   "metadata": {},
   "source": [
    "## 8.4 Defining the Optimum Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb5ba3-64d5-4d00-968b-fe6f57463296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this elbow graph to accurately determine the optimum number of clusters\n",
    "\n",
    "def elbow(census_merged_data, n):\n",
    "    kMeansVar = [KMeans(n_clusters=k).fit(census_merged_data.values) for k in range(1, n)] #making use of list comprehensions.\n",
    "    centroids = [X.cluster_centers_ for X in kMeansVar]\n",
    "    k_euclid = [cdist(census_merged_data.values, cent) for cent in centroids]\n",
    "    dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(census_merged_data.values)**2)/census_merged_data.values.shape[0]\n",
    "    bss = tss - wcss\n",
    "    plt.plot(bss)\n",
    "    plt.show()\n",
    " \n",
    "elbow(X,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ab192-5010-4a48-aac8-b1430253ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans with 3 clusters, after the validation with the Elbow method.\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.predict(X)\n",
    "cluster_centres = kmeans.cluster_centers_\n",
    "\n",
    "X['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54efcfd-814e-449d-869b-ae2058488e17",
   "metadata": {},
   "source": [
    "## 9. Evaluating and Interpreting Cluster Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1fefda-f7ff-4877-823f-0aff1e5eda39",
   "metadata": {},
   "source": [
    "**POINT VARIANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719a9423-5e8a-4a45-b517-034fd3542113",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "X['Cluster'] = clusters\n",
    "\n",
    "scaler = StandardScaler()\n",
    "stand_data_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA analysis.\n",
    "pca = PCA(n_components=2).fit(stand_data_scaled)\n",
    "pca_result = pca.transform(stand_data_scaled)\n",
    "\n",
    "#Percentage of variance explained by each of the selected components.\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Creating a scatter plot\n",
    "fig = px.scatter(x=pca_result[:, 0], y=pca_result[:, 1], color=clusters,\n",
    "                 labels={'color': 'Cluster'},\n",
    "                 #title='Cluster Plot against 1st 2 Principal Components',\n",
    "                 opacity=0.7,\n",
    "                 width=800, \n",
    "                 height=800)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "print(f\"These two components explain {(variance_ratio.sum()*100):.2f}% of the point variability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626ad21-8fd2-488e-8383-db5000feb14e",
   "metadata": {},
   "source": [
    "This is a high point variability, which we can see that there is less intermixing of the different cluster datapoints. This suggests that the K means clusters are fitting quite well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9c4d9-f959-4c09-9b85-2ed2e7977075",
   "metadata": {},
   "source": [
    "**POINT VARIANCE WITH 2 PRINCIPAL COMPONENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484b8b7-3d47-4d7f-84a6-08d469111a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a static figure with the point variability included in the x/y-axis label.\n",
    "# So we can see what variability is provided by each component.\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "X['Cluster'] = clusters\n",
    "\n",
    "# Standardize the data for PCA\n",
    "scaler = StandardScaler()\n",
    "stand_data_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2).fit(stand_data_scaled)\n",
    "pca_result = pca.transform(stand_data_scaled)\n",
    "\n",
    "#Percentage of variance explained by each of the selected components.\n",
    "variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=clusters, palette='viridis', s=50, alpha=0.7)\n",
    "plt.title('Cluster Plot against 1st 2 Principal Components')\n",
    "plt.xlabel(f'Principal Component 1 variation: {variance_ratio[0]*100:.2f}%')\n",
    "plt.ylabel(f'Principal Component 2 variation: {variance_ratio[1]*100:.2f}%')\n",
    "plt.legend(title='Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966e9e09-e280-4261-ac6e-7e7054414b1e",
   "metadata": {},
   "source": [
    "## 9.1 Interpreting the Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078293d-5a77-4d13-9780-5288a2179042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning a cluster value to each OA \n",
    "kmeans = KMeans(n_clusters=3, random_state=42) # Based on the elbow method, I'm using k = 3 \n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "# Get the cluster centers\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "\n",
    "cluster_centers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abee73d-6b08-4ea5-8267-28dd5eeed1b1",
   "metadata": {},
   "source": [
    "### Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be742bb2-836d-40a5-800d-7a51dc11506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster center values for the first row\n",
    "first_row_centers = cluster_centers.iloc[0, :]\n",
    "\n",
    "# Get the number of features\n",
    "num_features = len(first_row_centers)\n",
    "\n",
    "# Define the angles for polar coordinates\n",
    "theta = np.linspace(0, 2 * np.pi, num_features, endpoint=False)\n",
    "\n",
    "# Repeat the first value at the end to close the circle\n",
    "theta = np.append(theta, theta[0])\n",
    "first_row_centers = np.append(first_row_centers.values, first_row_centers.values[0])\n",
    "\n",
    "# Create the polar plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# Plot the cluster centers\n",
    "ax.plot(theta, first_row_centers, linewidth=1, color='blue', marker='o', label='Centers')\n",
    "\n",
    "# Plot the average line\n",
    "mean_value = np.mean(first_row_centers)\n",
    "ax.plot(theta, [mean_value] * len(theta), color='red', linestyle='--', label='Average')\n",
    "\n",
    "# Set the tick labels and rotation\n",
    "ax.set_xticks(theta[:-1])\n",
    "ax.set_xticklabels(cluster_centers.columns, rotation=90, ha='center')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f415b43-1ea6-4cf8-b8e6-88b788677bb6",
   "metadata": {},
   "source": [
    "### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3801bf-9fbd-41b4-b870-13dcadcfb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_row_centers = cluster_centers.iloc[1, :]\n",
    "\n",
    "# len of features\n",
    "num_features = len(second_row_centers)\n",
    "\n",
    "# polar coordinates\n",
    "theta = np.linspace(0, 2 * np.pi, num_features, endpoint=True)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.plot(theta, second_row_centers, linewidth=1, color='blue', marker='o', label='Centers')\n",
    "# Add an extra red line at the 0.0 value\n",
    "ax.plot(theta, np.zeros_like(second_row_centers), color='red', linestyle='--', label='Average')\n",
    "\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(cluster_centers.columns, rotation=90, ha='right')\n",
    "\n",
    "plt.show()\n",
    "#Ignore the cluster polar values, and focus in he census variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c828aed-f713-47ce-ac3d-4fc4e2f119bb",
   "metadata": {},
   "source": [
    "### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98216fe-64c9-4b83-b18c-e97bb12ea2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster 3 data\n",
    "third_row_centers = cluster_centers.iloc[2, :]\n",
    "\n",
    "# Number of features\n",
    "num_features = len(third_row_centers)\n",
    "\n",
    "# Angles for polar plot\n",
    "theta = np.linspace(0, 2 * np.pi, num_features, endpoint=False)\n",
    "theta = np.append(theta, theta[0])  # Close the loop\n",
    "third_row_centers = np.append(third_row_centers.values, third_row_centers.values[0])\n",
    "\n",
    "# Compute real average\n",
    "mean_value_2 = np.mean(third_row_centers)\n",
    "\n",
    "# Create polar plot\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "ax.plot(theta, third_row_centers, linewidth=1, color='blue', marker='o', label='Centres')\n",
    "ax.plot(theta, [mean_value_2]*len(theta), color='red', linestyle='--', label='Average')\n",
    "\n",
    "# Set axis ticks\n",
    "ax.set_xticks(theta[:-1])\n",
    "ax.set_xticklabels(cluster_centers.columns, rotation=45, ha='right')\n",
    "\n",
    "# Add legend and show\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
